\chapter{ماشین تورینگ عصبی}
در این فصل قرار است ساختار و محاسبات ماتع را مورد بررسی قرار دهیم. پس از آن مجموعه‌ای وظایف یادگیری ترتیبی\LTRfootnote{Sequence Learning} ساختگی که ماتع سنتی برای انجام آن توانمند است را معرفی خواهیم کرد.

\section{ساختار و محاسبات ماشین تورینگ عصبی}
ماتع شامل دو جز است:
\begin{enumerate}
\item کنترل‌گر\LTRfootnote{Controller}شبکه که می‌تواند یک شبکه عصبی جلورو \LTRfootnote{Feedforward Neural Network} یا یک شبکه عصبی بازگشتی باشد.
\item یک واحد حافظه خارجی که یک ماتریس  $N*W$ است. $N$ تعداد واحد‌های حافظه و $W$ ابعاد هر سلول حافظه را نمایش می‌دهد.\cite{collier2018implementing}
\end{enumerate}

فارغ از آنکه کنترل‌گر بازگشتی باشد یا خیر، کل معماری بازگشتی محسوب می‌شود چراکه ماتریس حافظه در طول زمان نگهداری می‌شود. کنترل‌گر سر\LTRfootnote{Head}های خواندن و نوشتن دارد که به ماتریس حافظه دسترسی دارد. تاثیر یک عمل خواندن یا نوشتن روی یک سلول حافظه خاص با مکانیسم توجه نرم\LTRfootnote{Soft Attention} وزن‌دهی می‌شود. این مکانیسم آدرس‌دهی مشابه مکانیسم توجه استفاده‌شده در یادگیری ماشین عصبی است به جز آنکه آدرس‌دهی وابسته به موقعیت را با آدرس‌دهی وابسته به محتوای موجود در مکانیسم توجه  را ترکیب می‌کند.\cite{collier2018implementing}
\\

به طور خاص برای یک ماتع در هر گام زمانی $t$ برای هر سر خواندن و نوشتن کنترل‌گر یک تعدادی پارامتر را به عنوان خروجی می‌دهد. این پارامتر‌ها برای محاسبه وزن $w_t$ بر روی $N$ خانه حافظه در ماتریس حافظه $M_t$ استفاده می‌شوند. نحوه محاسبه $w_t^c$ یعنی آدرس‌دهی وابسته به محتوا در رابطه ۲-۱ آورده شده است.\cite{collier2018implementing}

\begin{equation}
w^c_t(i) = \frac{exp(\beta_t K[k_t,M_t(i)])}{\sum_{j=0}^{N-1} exp(\beta_t K[k_t,M_t(j)])}
\end{equation}

در رابطه ۲-۱ $k_t$ یک کلید جستجو در حافظه را نشان می‌دهد و $K$ مطابق رابطه ۲-۲ یک معیار شباهت\LTRfootnote{Similarity Metric} مانند شباهت کسینوسی\LTRfootnote{Cosine Similarity} است.\cite{collier2018implementing}

\begin{equation}
K[u, v] = \frac{u·v}{||u||.||v||} 
\end{equation}

با یک سری از محاسبات مطابق روابط ۲-۳، ۲-۴ و ۲-۵ ماتع‌ها امکان تکرار بر روی وزن‌های حافظه فعلی و قبلا محاسبه‌شده را خواهند داشت. رابطه ۲-۳ به شبکه اجازه می‌دهد تا بین بردار وزن قبلی یا فعلی انتخاب کند که از کدام استفاده کند. رابطه ۲-۴ امکان تکرار از طریق حافظه با عمل کانولوشن وزن فعلی و یک هسته\LTRfootnote{Kernel}، کانوولوشنی جابجایی\LTRfootnote{Shift} یک بعدی را فراهم می‌کند. رابطه ۲-۵ رخداد تارشدن\LTRfootnote{Blurring} که به واسطه عمل کانوولوشن رخ داده است را اصلاح می‌کند.\cite{collier2018implementing}


\begin{equation}
w^g_t = g_t w^c_t + (1 - g_t)w_{t-1}
\end{equation}

\begin{equation}
\tilde{w}_t(i) = \sum^{N-1}_{j=0} w^g_t(j)s_t(i - j)
\end{equation}

\begin{equation}
w_t(i) = \frac{\tilde{w}_t(i)^{\gamma_t}}{\sum_{j=0}^{N-1}\tilde{w}_t(j)^{\gamma_t}}
\end{equation}

سپس بردار $r_t$ مطابق رابطه ۲-۶ به وسیله‌ی یک سر خواندن خاص در زمان $t$ محاسبه می‌گردد.\cite{collier2018implementing}

\begin{equation}
r_t = \sum_{i=0}^{N-1} w_t(i)M_t(i)
\end{equation}

نهایتا مطابق رابطه ۲-۷ و ۲-۸ هر سر نوشتن ماتریس حافظه را در گام $t$ با محاسبه بردارهای جانبی پاک‌کردن یعنی $e_t$ و جمع‌‌کردن یعنی $a_t$ تغییر می‌دهد.\cite{collier2018implementing}

\begin{equation}
\tilde{M}_t(i) = M_{t-1}(i)[1-w_t(i)e_t]
\end{equation}

\begin{equation}
M_t(i) = \tilde{M}_t(i) + w_t(i)a_t
\end{equation}

در روابط این بخش برخی از پارامترها دارای محدودیت‌هایی هستند که باید به آن اشاره شود:\cite{collier2018implementing}
\begin{itemize}
\item $\beta_t \ge 0$
\item $g_t \in [0, 1]$
\item $\sum_k s_t(k) = 1$
\item $\forall_k s_t(k) \ge 0$
\item $\gamma_t \ge 1$
\end{itemize}


\section{وظایف یادگیری ترتیبی ساختگی}
برای ماتع‌ها چندین وظیفه ساختگی در نظر گرفته شده است که تمام آن‌ها از نوع مسئله یادگیری ترتیبی است؛ زمینه‌ای که آن‌ها در آن توانمند هستند. این وظایف عبارت اند از:
\begin{itemize}
\item \textbf{رونوشت‌گیری}:
برای وظیفه رونوشت‌گیری یک دنباله تصادفی از بردارهای بیت با یک نماد خاص به عنوان پایان دنباله به شبکه داده می‌شود. این وظیفه نیاز دارد تا شبکه دنباله ورودی را نگه دارد و سپس آن را از حافظه برگرداند.

\item \textbf{رونوشت‌گیری تکرارشونده}\LTRfootnote{Repeat Copy}:
مشابه وظیفه رونوشت‌گیری دنباله‌ای از بردارهای بیتی تصادفی به شبکه داده می‌شود. برخلاف وظیفه رونوشت‌گیری بعد از اتمام دنباله یک عدد که نشان دهنده تعداد دفعاتی است که باید دنباله در خروجی ظاهر شود به شبکه داده می‌شود.

\item \textbf{یادآوری انجمنی}\LTRfootnote{Associative Recall}:
این وظیفه دارای دنباله با دنباله‌های متشکل از بردارهای بیتی تصادفی است. بعد از آنکه یک دنباله از آیتم‌ها و نماد پایانی دنباله به شبکه داده می‌شود. خروجی صحیح عنصر بعدی دنباله ورودی بعد از عنصر کوئری است.\cite{collier2018implementing} 

\end{itemize}