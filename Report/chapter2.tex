\chapter{ماشین تورینگ عصبی}
\section{ساختار و محاسبات ماشین تورینگ عصبی}
ماتع شامل دو جز است:
\begin{enumerate}
\item کنترل‌گر شبکه که می‌تواند یک شبکه‌عصبی جلورو یا یک شبکه عصبی بازگشتی باشد.
\item یک واحد حافظه خارجی که یک ماتریس حافظه $N*W$ است. $N$ تعداد واحد‌های حافظه و $W$ ابعاد هر سلول حافظه را نمایش می‌دهد.\cite{collier2018implementing}
\end{enumerate}

فارغ از آنکه کنترل‌گر بازگشتی باشد یا خیر، کل معماری بازگشتی محسوب می‌شود چراکه ماتریس حافظه در طول زمان نگهداری می‌شود. کنترل‌گر سرهای خوانده و نوشتن دارد که به ماتریس حافظه دسترسی دارد. تاثیر یک عمل خواندن یا نوشتن روی یک سلول حافظه خاص با مکانیسم توجه نرم\LTRfootnote{Soft Attention Mechanism} وزن‌دهی می‌شود. این مکانیسم آدرس‌دهی مشابه مکانیسم توجه استفاده‌شده در یادگیری ماشین عصبی است به جز آنکه آدرس‌دهی وابسته به موقعیت را با آدرس‌دهی وابسته به محتوای موجود در مکانیسم توجه  را ترکیب می‌کند.\cite{collier2018implementing}
\\

به طور خاص برای یک ماتع در هر گام زمانی $t$ برای هر سر خواندن و نوشتن کنترل‌گر یک تعدادی پارامتر را به عنوان خروجی می‌دهد. این پارامتر‌ها برای محاسبه وزن $w_t$ بر روی $N$ خانه حافظه در ماتریس حافظه $M_t$ استفاده می‌شوند. نحوه محاسبه $w_t$ در رابطه ۲-۱ آورده شده است.\cite{collier2018implementing}

\begin{equation}
w^c_t(i) \leftarrow \frac{exp(\beta_t K[k_t,M_t(i)])}{\sum_{j=0}^{N-1} exp(\beta_t K[k_t,M_t(j)])}
\end{equation}

در رابطه ۲-۱ برخی از پارامترها دارای محدودیت‌هایی هستند:\cite{collier2018implementing}
\begin{itemize}
\item $\beta_t \le 0$
\item $g_t \in [0, 1]$
\item $\sum_k s_t(k) = 1$
\item $\forall_k s_t(k) \le 0$
\item $\gamma_t \le 1$
\end{itemize}

در رابطه ۲-۱ $w_t^c$ آدرس‌دهی وابسته به محتوا را فراهم می‌کند. $k_t$ یک کلید جستجو در حافظه را نشان می‌دهد و $K$ مطابق رابطه ۲-۲ یک معیار شباهت مانند شباهت کسینوسی است.\cite{collier2018implementing}

\begin{equation}
K[u, v] = \frac{u·v}{||u||.||v||} 
\end{equation}

با یک سری از محاسبات مطابق روابط ۲-۳، ۲-۴ و ۲-۵ ماتع‌ها امکان تکرار بر روی وزن‌های حافظه فعلی و قبلا محاسبه‌شده را خواهند داشت. رابطه ۲-۳ به شبکه اجازه می‌دهد تا بین بردار وزن قبلی یا فعلی انتخاب کند که از کدام استفاده کند. رابطه ۲-۴ امکان تکرار از طریق حافظه با عمل کانولوشن وزن فعلی و یک کرنل کانوولوشنی جابجایی\LTRfootnote{Shift} یک بعدی را فراهم می‌کند. رابطه ۲-۵ رخداد تارشدن\LTRfootnote{Blurring} که به واسطه عمل کانوولوشن رخداده است را اصلاح می‌کند.\cite{collier2018implementing}


\begin{equation}
w^g_t \leftarrow g_t w^c_t + (1 − g_t)w_{t−1}
\end{equation}

\begin{equation}
\tilde{w}_t(i) \leftarrow \sum^{N-1}_{j=0} w^g_t(j)s_t(i − j)
\end{equation}

\begin{equation}
w_t(i) \leftarrow \frac{\tilde{w}_t(i)^{\gamma_t}}{\sum_{j=0}^{N-1}\tilde{w}_t(j)^{\gamma_t}}
\end{equation}

سپس بردار $r_t$ مطابق رابطه ۲-۶ به وسیله‌ی یک سر\LTRfootnote{Head} خواندن خاص در زمان $t$ محاسبه می‌گردد.\cite{collier2018implementing}

\begin{equation}
r_t \leftarrow \sum_{i=0}^{N-1} w_t(i)M_t(i)
\end{equation}

نهایتا مطابق رابطه ۲-۷ و ۲-۸ هر سر نوشتن ماتریس حافظه را در گام $t$ با محاسبه بردارهای جانبی پاک‌کردن یعنی $e_t$ و جمع‌‌کردن یعنی $a_t$ تغییر می‌دهد.\cite{collier2018implementing}

\begin{equation}
\tilde{M}_t(i) \leftarrow M_{t-1}(i)[1-w_t(i)e_t]
\end{equation}

\begin{equation}
M_t(i) \leftarrow \tilde{M}_t(i) + w_t(i)a_t
\end{equation}

\section{وظایف یادگیری ترتیبی}
برای ماتع‌ها چندین وظیفه مصنوعی در نظر گرفته شده است که تمام آن‌ها از نوع مسئله یادگیری ترتیبی\LTRfootnote{Sequence Learning} است؛ زمینه‌ای که آن‌ها در آن توانمند هستند. این وظایف عبارت اند از:
\begin{itemize}
\item \bf{رونوشت‌گیری}\LTRfootnote{Copy}:
برای وظیفه رونوشت‌گیری یک دنباله تصادفی از بردارهای بیت با یک نماد خاص به عنوان پایان دنباله به شبکه داده می‌شود. این وظیفه نیاز دارد تا شبکه دنباله ورودی را نگه دارد و سپس آن را از حافظه برگرداند.

\item \bf{رونوشت‌گیری تکرارشونده}\LTRfootnote{Repeat Copy}:
مشابه وظیفه رونوشت‌گیری دنباله‌ای از بردارهای بیتی تصادفی به شبکه داده می‌شود. برخلاف وظیفه رونوشت‌گیری بعد از دنباله یک عدد که نشان دهنده تعداد دفعاتی است که باید دنباله در خروجی ظاهر شود به شبکه داده می‌شود.

\item \bf{یادآوری انجمنی}\LTRfootnote{Associative Recall}:
این وظیفه نیز یک مسئله یادگیری دنباله با دنباله‌های متشکل از بردارهای بیتی تصادفی است. در این مورد ورودی به چندین عنصر تقسیم می‌شود که هر کدام شامل بردارهای ۳×۶ بعدی است. بعد از آنکه یک دنباله از آیتم‌ها و نماد پایانی دنباله به شبکه داده می‌شود. خروجی صحیح عنصر بعدی دنباله ورودی بعد عنصر کوئری است.\cite{collier2018implementing} 

\end{itemize}