\chapter{افزونه‌ها}

\section{ماشین تورینگ عصبی تکاملی}
تکامل عصبی توپولوژی‌های تقویت‌کننده\LTRfootnote{Neuroevolution of Augmenting Topologies(NEAT)} که در ادامه آن را به اختصار تکتوت می‌نامیم با یک جمعیت\LTRfootnote{Population} از شبکه‌های عصبی ساده شروع می‌کند و سپس آن‌ها را در طی نسل‌ها\LTRfootnote{Generations} با افزودن رئوس جدید و اتصالات به کمک جهش\LTRfootnote{Mutation} پیچیده‌تر می‌کند. با تکامل شبکه‌ها از این را لازم نیست توپولوژی شبکه‌ها از پیش دانسته شده باشد. تکتوت به طرز فزاینده‌ای در شبکه‌های پیچیده جستجو می‌کند تا یک سطح مناسب از پیچیدگی را پیدا کند. ویژگی مهم تکتوت این است که هم توپولوژی و هم وزن‌های یک شبکه را تکامل می‌دهد. چراکه به سادگی و تدریجی پیچیدگی را افزایش می‌دهد و این باعث می‌شود که یک شبکه مناسب با اندازه مینیمال حاصل شود.\cite{merrild2018hyperntm}
\\

بر پایه ماتع و با استفاده از تکتوت مدل ماشین تورینگ عصبی تکاملی\LTRfootnote{Evolvable Neural Turing Machine} که در ادامه آن را به طور اختصار ماتع تکاملی می‌نامیم معرفی شده است. در این روش توپولوژی و وزن‌های شبکه عصبی کنترل‌گر با کمک تکتوت یاد گرفته می‌شود. بنابراین برخلاف ماتع استاندارد نیاز به دانش پیشین\LTRfootnote{Prior Knowledge} نیست و شبکه می‌تواند باتوجه به پیچیدگی وظیفه رشد پیدا کند. ماتع تکاملی اغلب توپولوژی‌‌های فشرده برای حل یک وظیفه خاص پیدا می‌کند؛ در نتیجه جلوی جستجوی غیرضروری در فضای با ابعاد بالا گرفته می‌شود. به علاوه ماتع تکاملی قادر به جل مسائل یادگیری مستمر پیچیده است. چراکه شبکه از مشتق استفاده نمی‌کند و می‌تواند از توجه سخت\LTRfootnote{Hard Attention} و مکانیسم جابجایی\LTRfootnote{Shift Mechanism} استفاده کند که امکان تعمیم خوب برای دنباله‌های بلند در وظیفه رونوشت‌گیری را فراهم می‌کند. به علاوه یک نوار\LTRfootnote{Tape} پویا و از نظر تئوری با اندازه بی‌نهایت قابل استفاده است.\cite{merrild2018hyperntm}
\\

ماتع تکاملی یک سر تکی ترکیبی خواندن/نوشتن دارد. این شبکه بردار نوشتن $w$ با اندازه $M$، ورودی کنترل درون‌یابی\LTRfootnote{Interpolation} نوشتن $i$، ورودی کنترل پرش محتوا $j$ و سه ورودی کنترل جابجایی $s_l$ (جابجایی چپ)، $s_0$ (بدون جابجایی) و $s_r$ (جابجایی راست) را خروجی می‌دهد. اندازه بردار نوشتن $M$ اندازه هر خانه حافظه بر روی نوار را مشخص می‌کند. جزء درون‌یابی نوشتن امکان مخلوط‌کردن مقادیر فعلی نوار و بردار نوشتن در موقعیت نوشتن را فراهم می‌کند. $M_h(t)$ محتوای نوار در موقعیت سر فعلی $h$ در زمان $t$، $i_t$ درون‌یابی نوشتن و $w_t$ بردار نوشتن است. برای تمام این‌ها در زمان $t$ رابطه ۳-۱ را خواهیم داشت.\cite{merrild2018hyperntm}

\begin{equation}
M_h(t) = M_h(t − 1) · (1 − i_t) + w_t · i_t
\end{equation}

پرش محتوا مشخص می‌کند که آیا سر باید به موقعیتی در حافظه حرکت کند که بیشترین شباهت را به بردار نوشتن دارد یا نه. یک پرش محتوا انجام می‌شود اگر مقدار ورودی کنترل از ۰.۵ بیشتر شود. شباهت بین بردار نوشتن $w$ و بردار حافظه $m$‌ مطابق با رابطه ۳-۲ حساب می‌شود.\cite{merrild2018hyperntm}

\begin{equation}
s(w, m) = \frac{\sum_{i=1}^{M} |w_i−m_i| }{M} 
\end{equation}

در گام زمانی $t$ اقدامات زیر به ترتیب انجام می‌شود:
\begin{enumerate}
\item بردار نوشتن $w_t$ برای موقعیت فعلی $h$ بدست می‌آید. این بردار با محتوای موجود باتوجه به درون‌یابی نوشتن $i_t$ درون‌یابی می‌شود.
\item اگر ورودی کنترل پرش محتوا $j_t$ بزرگ‌تر از ۰.۵ شود، سر به مکانی در نوار که بیشترین شباهت به بردار نوشتن $w_t$ دارد حرکت می‌کند.
\item سر به یک موقعیت چپ‌تر، راست‌تر روی نوار حرکت می‌کند یا در همان جا ثابت می‌ماند که این وابسته به مقادیر ورودی کنترل جابجایی $s_l$، $s_0$  و $s_r$ است.
\item مقادیر نوار را در موقعیت جدید سر می‌خواند و بر می‌گرداند.\cite{merrild2018hyperntm}
\end{enumerate}

\section{ماشین تورینگ عصبی ابر تکاملی}
در کدگذاری‌های مستقیم مانند تکتوت هر بخش از نمایش جواب به یک تکه کوچک از ساختار نهایی جواب نگاشت می‌شود. عیب مهم این روش آن است که بخش‌های مختلف راه‌حل که به یک‌دیگر شبیه هستند نیز باید کد شوند و جداگانه کشف شوند. این ابراد با کدگذاری غیرمستقیم تا حد زیادی قابل حل است؛ در کدگذاری غیرمستقیم راه‌حل به شکل فشرده توصیف می‌شود و حجم اطلاعات کدشده می‌تواند کاهش بیابد. در کدگذاری غیرمستقیم به دلیل آنکه یک راه‌حل به شکل الگویی از پارامترها و نه تمام پارامترها نمایش پیدا می‌کند قدرت‌مند است.\cite{merrild2018hyperntm}
\\

روش ابر تکامل عصبی توپولوژی‌های تقویت‌کننده\LTRfootnote{Hyper Neuroevolution of Augmenting Topologies(HyperNEAT)} که در ادامه به اختصار آن را ابرتوت می‌نامیم یک افزونه از تکتوت است. در این افزونه به جای کدگذاری غیرمستقیم از کدگذاری مستقیم استفاده می‌شود. در تکتوت از شبکه‌های عصبی معمولی استفاده می‌شود درحالی که در ابرتوت از شبکه‌های تولید الگوی ترکیبی\LTRfootnote{Compositional Pattern Producing Networks (CPPN)} که در ادامه آن را به اختصار شبکه تات می‌نامیم استفاده شده است. شبکه تات برای کدگذاری ترکیب توابع طراحی شده‌اند که هر تابع در ترکیب مرتبط با یک منظم‌سازی\LTRfootnote{Regularity} است.\cite{merrild2018hyperntm}
\\

حسن شبکه تات آن است که به الگوهای مکانی اجازه می‌دهد که به عنوان شبکه‌هایی از توابع ساده نمایش پیدا کنند. این یعنی تکتوت می‌تواند با شبکه تات مانند شبکه‌های عصبی تکامل پیدا کند. شبکه‌های تات مشابه شبکه‌های عصبی هستند با این تفاوت که آن‌ها متکی بر بیشتر از یک تابع فعال‌سازی هستند.
کدگذاری غیرمستقیم شبکه تات می‌تواند به طور فشرده الگوها با نظم‌هایی نظیر تقارن\LTRfootnote{Symmetry}، تکرار\LTRfootnote{Repetition} و تکرار با تغییر\LTRfootnote{Repetition with Variation} را کد کنند. به عنوان مثال با انتخاب یک تابع گاوسین که خاصیت تقارن دارد الگوی خروجی نیز به سادگی متقارن خواهد شد. انتخاب یک تابع دوره‌ای مانند سینوس در حین تکرار قطعه‌سازی انجام می‌دهد. نهایتا تکرار با تغییر به سادگی با ترکیب یک تابع منظم (مانند سینوس یا گاوسین) با یک تابع نامنظم (مانند محور \lr{x} نامتقارن) بدست می‌آید.\cite{merrild2018hyperntm} 
\\

ایده اصلی ابرتوت آن است که شبکه‌های تات می‌تواند اتصال الگوها را کدکند. بدین طریق یک تکتوت می‌تواند یک شبکه تات که شبکه‌های عصبی بزرگ با منظم‌سازی‌ها و تقارن‌های خود نمایش می‌دهد را تکامل دهد.\cite{merrild2018hyperntm}
\\

عملکرد شبکه تات در تصویر ۳-۱ آورده شده است. شبکه‌های تات سنتی توابع هندسی هستند که الگوهای اتصال خروجی آن رئوسی در $n$ بعد است که $n$ تعداد ابعاد در فضای کارتزین است. یک شبکه تات که چهار ورودی با برچسب‌های $x_1$، $y_1$، $x_2$ و $y_2$ را دریافت کند و به عنوان خروجی مشخص می‌کند که اتصال بین نقاط دوبعدی $(x_1, y_1)$ و $(x_2, y_2)$ چه میزان است. بنابراین با داشتن یک شبکه تات آموزش یافته می‌توان با ارسال یک کوئری شامل هر دو راس در شبکه عصبی مقدار اتصال آن را بدست آورد و شبکه عصبی را ایجاد کرد.\cite{merrild2018hyperntm}

\begin{figure}[!h]
\begin{center}
\includegraphics[height=7cm]{CPPN.png}
\end{center}
\caption{نحوه کارکرد شبکه تات\cite{merrild2018hyperntm} }
\medskip
\small
مجموعه‌ای از گره‌ها به مختصات بین -۱ تا +۱ در تمام ابعاد نظیر می‌شوند.
(۱): هر اتصال ممکن در یک شبکه عصبی کوئری زده می‌شود تا مجاورت و وزن آن مشخص شود. خطوط جهت‌دار تیره نمایش‌داده شده در تصویر یک نمونه از اتصالاتی است که کوئری زده شده است.
(۲): در درون یک شبکه تات یک گراف است که مشخص می‌کند کدام توابع فعال‌سازی به یکدیگر متصل هستند. همان‌طور که در شبکه عصبی اتصالات وزن‌دهی می‌شوند که خروجی یک تابع با چه وزنی به طرف دیگر اتصال برود، برای هر کوئری ارسال‌شده به شبکه تات جایگاه دو سر اتصال را به عنوان ورودی می‌گیرد و وزن اتصال را به عنوان خروجی می‌دهد. (۳): بنابراین شبکه تات می‌تواند الگوهای منظم از اتصالات در فضا را تولید کند.

\end{figure}

همانطور که در توضیحات قبل بدان اشاره شد باید تمام گره‌های شبکه عصبی در یک بستر\LTRfootnote{Substrate} قرار داده شوند. یعنی مشخص شود که هر گره در شبکه عصبی در چه مختصاتی در فضا باید قرار بگیرد. مشابه چیزی که در سمت چپ شکل ۳-۱ قابل مشاهده است. ایده اصلی در ماتع ابرتکاملی پیشنهاد یک بستر برای بهره‌مندی از شبکه تات در ماتع تکاملی بوده است.
\\

در مدل ماتع ابرتکاملی شبکه تات نه تنها اتصال ورودی‌ها و خروجی‌های شبکه عصبی مرتبط با وظیفه را مشخص می‌کند بلکه اینکه اطلاعات آمده از حافظه چگونه باید در شبکه ادغام شوند و چگونه اطلاعات در حافظه نوشته شوند را هم مشخص می‌کند. زیرا ابرتوت که می‌تواند هندسه یک وظیفه را یاد بگیرد قائدتا باید بتواند الگوی هندسی اطلاعات خوانده شده از و نوشته شده در حافظه را نیز یاد بگیرد.\cite{merrild2018hyperntm}
\\

شبکه ماتع تکاملی ورودی‌های زیر را دارد:
\begin{itemize}
\item \bf{شروع}: ورودی که هرگاه فعال می‌شود، ذخیره اعداد شروع می‌شود.
\item \bf{تعویض}: ورودی که هرگاه فعال شود، ذخیره اطلاعات خاتمه می‌یابد و شبکه باید مقادیر به خاطر سپرده‌شده را به یاد بیاورد.
\item \bf{ورودی بردار بیتی}: بردار بیتی که شبکه به عنوان ورودی می‌گیرد. توجه کنید قبل از آنکه ورودی تعویض فعال شود رنج این ورودی با بیت‌هایی که بعدا می‌خواند فعال می‌شود.
\item \bf{ورودی خواندن حافظه}: بردار حافظه که ماشین تورینگ در گام قبل خوانده است.\cite{merrild2018hyperntm}
\end{itemize}

این شبکه خروجی‌های زیر را هم دارد:
\begin{itemize}
\item \bf{خروجی بردار بیتی}: بردار بیتی که شبکه به عنوان خروجی می‌دهد. توجه کنید در حین دریافت ورودی این خروجی نادیده گرفته می‌شود.
\item \bf{خروجی نوشتن حافظه}: بردار حافظه‌ای که باید در حافظه نوشته شود.
\item \bf{کنترل‌گر‌های ماشین تورینگ}: خروجی‌های کنترل مخصوص ماشین تورینگ یعنی پرش، درون‌یابی و سه کنترل جابجایی (چپ، راست و توقف)\cite{merrild2018hyperntm} 
\end{itemize}

در ادامه بستر طراحی‌شده برای وظیفه رونوشت‌گیری آورده می‌شود. این بستر در شکل ۳-۲ نشان داده شده است. این بستر طراحی شده است که گره‌های ورودی بردار بیتی مختصات \lr{x} را با گره‌های نوشتن بردار حافظه به اشتراک بگذارد و بالعکس با گره‌های خواندن بردار حافظه و گره‌های خروجی بردار بیتی.
به علاوه ورودی تعویض مختصات \lr{x} اش را با خروجی پرش به اشتراک می‌گذارد بنابراین شبکه را می‌تواند وادار به پرش به حافظه‌ای کند که خواندن را از آن شروع کرده است. در این مقاله اندازه بردارهای حافظه برابر با اندازه بردار بیتی است. 
به علاوه هیچ یک از بسترها شامل گره‌های مخفی مانند آن چیزی که نشان داده شده است و ممکن است مسائل  با اندازه‌های بزرگ‌تر بدون گره مخفی را حل کند نیست.\cite{merrild2018hyperntm}


\begin{figure}[!h]
\begin{center}
\includegraphics[height=5cm]{HyperENTM-Substrate.png}
\end{center}
\caption{بستر طراحی‌شده برای شبکه تات در مدل ماتع ابرتکاملی\cite{merrild2018hyperntm} }
\medskip
\small
تمام ورودی‌ها در $z=1$ و تمام خروجی‌ها در $z=-1$ هستند. قسمت الف تمام گره‌ها در $y=1$ را نشان می‌دهد که ورودی‌های شروع، تعویض و کنترل‌‌گرهای ماشین تورینگ هستند. لازم به ذکر است که مختصات $x$ برای ورودی تعویض و خروجی کنترل‌گر پرش یکسان است.
قسمت ب گره‌ها را در $y=-1‌$ نشان می‌دهد که ورودی و خروجی‌های بردار حافظه و بردار بیتی را نشان می‌دهد. گره‌های ورودی بردار بیتی مختصات $x$ را با گره‌های نوشتن بردار حافظه به اشتراک می‌گذارد، درحالی‌که گره‌های خواندن بردار حافظه مختصات $x$ را با گره‌های خروجی بردار بیتی به اشتراک می‌گذارد.\cite{merrild2018hyperntm}
\end{figure}

در کنار خروجی شبکه تات که وزن هر اتصال را مشخص می‌کند، هر شبکه تات یک خروجی تابع قدم\LTRfootnote{Step Function} اضافه دارد که خروجی بیان پیوند نامیده می‌شود. این خروجی مشخص می‌کند که آیا یک اتصال باید بیان شود یا خیر. اتصالات بالقوه برای هر ورودی در لایه‌های $y=1$ و $y=-1$ به هر خروجی در لایه‌های $y=1$ و $y=-1‌$ کوئری زده می‌شود. تعداد ورودی‌ها و خروجی‌ها در لایه $y=-1$ مطابق قسمت ب شکل ۳-۲ وابسته به اندازه بردار بینی وظیفه رونوشت‌گیری است، که در مثال نشان‌داده شده اندازه بردار بیتی برابر با ۳ است. نورون‌ها به شکل یکنواخت در بازه‌های -۱ تا -۰.۲ در مختصات $x$ برای ورودی‌های بردار بیتی و بردار نوشتن حافظه استفاده می‌شود و بازه ۰.۲ تا ۱ برای بردار نوشتن حافظه و خروجی بردار بیتی استفاده می‌شود.\cite{merrild2018hyperntm} 
\\

نهایتا برای تعیین میزان بایاس یک گره باید مختصات همان گره را هم به عنوان گره مبدا و هم به عنوان گره مقصد به شبکه تات کوئری زد.\cite{merrild2018hyperntm}

\section{ماشین تورینگ عصبی متوجه}
در مدل ماتع متوجه حافظه با یک مکانیسم آدرس‌دهی وابسته به محتوا بازیابی می‌شود. آدرس‌دهی برپایه محتوا به صورت ضروری یک گام محاسبه شباهت میان بردار خروجی کنترل‌گر $C_t$ و بردارهای حافظه موجود $M_t$ است. وزن توجه در خواندن با رابطه ۳-۳ تولید می‌شود. در رابطه مذکور متغیر $\beta$ می‌تواند دقت تمرکز را افزایش یا کاهش دهد و معیار شباهت استفاده‌شده معیار شباهت کسینوسی است.\cite{zhao2020cold}

\begin{equation}
w_t^r(i) = \frac{exp(\beta.sim(C_t,M_t(i)))}{\sum_j(exp(\beta.sim(C_t,M_t(j))))}
\end{equation}

در حالت کلی کنترل‌گر می‌تواند با هر شبکه عصبی مصنوعی طراحی شود. در ماتع متوجه از یک لایه توجه چندسر\LTRfootnote{Multi-Headed Attention} استفاده کرده‌اند تا روابط بین دنباله‌های ورودی و خروجی را مدل کنند. \cite{zhao2020cold}
\\

رابطه مربوط به لایه توجه در رابطه ۳-۴ آورده شده است. در این رابطه مطابق معمول $K$، $Q$ و $V$ به ترتیب ماتریس کوئری، کلید و مقدار است و $n$ تعداد ابعاد است. توجه بر دنباله ورودی $I$ اعمال می‌شود تا یک جمع وزن‌دار برای هر $i_t$ بر پایه $i_1$ تا $i_t$ اعمال شود که $t$ زمان فعلی است.\cite{zhao2020cold} 

\begin{equation}
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{n}})
\end{equation}

به عنوان خروجی مدل یک بردار وزن مرتبط با هر تلاش را یاد می‌گیرد و بردار وزن یادگرفته‌شده برای محاسبه پیش‌بینی خروجی $i_t$ استفاده می‌شود. از تابع خطا میانگین مربعات خطا مطابق رابطه ۳-۵ استفاده می‌شود. در این رابطه $a_t^k$ مقدار پیش‌بینی، $gt_t^k$ مقدار واقعی برچسب، $b$ اندازه دسته\LTRfootnote{Batch}، $k$ طول دنباله و $t$ زمان است.\cite{zhao2020cold}

\begin{equation}
L = \sum_b \sum_{k=1}^n \sum_{t=1}^T MSE(a_t^k, gt_t^k)
\end{equation}

در ماتع متوجه حافظه با مکانیسم آدرس‌دهی برپایه موقعیت بروز می‌شود. مکانیسم آدرس‌دهی برپایه موقعیت برای چندین گام طراحی شده است تا تکرار بر روی خانه‌های حافظه و پرش‌های دسترسی تصادفی را امکان‌پذیر کند. گام اول محاسبه درون‌یابی بین بردار وزن نوشتن پیشین $w_{t-1}$ و بردار وزن محتوا تولیدشده توسط مکانیسم آدرس‌دهی محتوا $w_t^c$ در گام زمانی فعلی با استفاده از دروازه\LTRfootnote{Gate} درون‌یابی $w_t^g$ مطابق رابطه ۳-۶ است.\cite{zhao2020cold}

\begin{equation}
w^g_t = g_t ∗ w^c_t + (1−g_t) ∗ w_{t−1}
\end{equation}

بعد از درون‌یابی یک وزن‌دهی جابجایی $s_t$ بر ماتریس وزن دروازه‌دار با یک کانولوشن دایره‌ای برای تنظیم حافظه مطابق رابطه ۳-۷ اعمال می‌شود. در این رابطه $N$ برابر با اندازه حافظه است. نهایتا مطابق رابطه ۳-۸ عمل تیزکردن\LTRfootnote{Sharpen} برای نرمال‌سازی استفاده می‌شود.\cite{zhao2020cold}

\begin{equation}
w_t^{ro} = \sum_{j=0}^{N-1}w_t^g(j)s_t(i-j)
\end{equation}

\begin{equation}
w_t(i) = \frac{w_t^{ro}(i)}{\sum_j w_t^{ro}(j)}
\end{equation}

\section{ماشین تورینگ عصبی پویا}

ماتع سنتی دو مدل آدرس‌دهی را پشتیبانی می‌کند که می‌تواند به صورت همزمان استفاده شود: آدرس‌دهی بر پایه محتوا و آدرس‌دهی بر پایه موقعیت. استراتژی برپایه موقعیت خود بر پایه آدرس‌دهی خطی است که فاصله بین یک جفت از سلول حافظه متوالی همواره برابر با یک مقدار ثابت است. ماتع پویا این محدودیت را با معرفی یک بردار آدرس‌دهی قابل یادگیری برای هر سلول حافظه در ماتع که اخیرا به عنوان مکانیسم آدرس‌دهی حافظه استفاده شده است حل کرده است.\cite{gulcehre2018dynamic}

\subsection{معماری ماشین تورینگ عصبی پویا}
ماتع پویا شامل یک حافظه خارجی $M_t$ است که هر سلول حافظه $i$ در $M_t[i]$ به دو بخش شکسته می‌شود: 
یک بردار آدرس آموزش‌پذیر $A_t[i]$ و بردار محتوا $C_t[i]$ است. به طور مشابه می‌توان دید که کل حافظه به یک ماتریس آدرس آموزش‌پذیر $A_t$ و یک ماتریس محتوای $C_t$ شکسته می‌شود. بنابراین برای هر سلول حافظه رابطه ۳-۹ و برای کل حافظه رابطه ۳-۱۰ برقرار خواهد بود.\cite{gulcehre2018dynamic}

\begin{equation}
M_t[i] = [A_t[i]; C_t[i]]
\end{equation}

\begin{equation}
M_t = [A_t; C_t]
\end{equation} 

بخش آدرس $A_t$ پارامتر مدل است که در طول یادگیری بروز می‌شود. در حین استنتاج بخش آدرس توسط کنترل‌گر تغییر پیدا نمیکند و ثابت می‌ماند. بخش محتوا $C_t$ در حین آموزش و استنتاج توسط کنترل‌گر چه برای نوشتن و چه برای خواندن تغییر پیدا می‌کند. در ابتدای هر دوره بخش محتوای حافظه به یک ماتریس تمام صفر تغییر پیدا می‌کند. ($C_0 = 0$)این شروع باعث می‌شود که بخش قابل آموزش آدرس برای هر سلول حافظه به مدل امکان یادگیری استراتژی‌های آدرس‌دهی پیچیده مبتنی بر مکان را بدهد.\cite{gulcehre2018dynamic}

کنترل‌گر در هر گام زمانی $t$ اقدامات زیر را انجام می‌دهد:
\begin{enumerate}
\item یک مقدار ورودی $x_t$ دریافت می‌کند.
\item به حافظه دسترسی پیدا می‌کند و آن را می‌خواند و بردار محتوای $r_t$ را ایجاد می‌کند.
\item یک بخشی از اطلاعات را روی حافظه می‌نویسد.
\item وضعیت تصادفی خود را بروز می‌کند.
\item در صورت نیاز مقدار $y_t$ را به عنوان خروجی می‌دهد.
\end{enumerate}

در ماتع پویا هم امکان استفاده از کنترل‌گر جلورو و هم استفاده از کنترل‌گر \lr{GRU} وجود دارد. نحوه محاسبه وضعیت مخفی با استفاده از این دو کنترل‌گر به ترتیب در رابطه ۳-۱۱ و ۳-۱۲ آمده است.\cite{gulcehre2018dynamic}
\begin{equation}
h_t = \sigma(x_t, r_t)
\end{equation}
\begin{equation}
h_t = GRU(x_t, h_{t-1}, r_t)
\end{equation}

در گام زمانی $t$ کنترل‌گر $x_t$‌ را به عنوان ورودی می‌گیرد و سپس وزن خواندن $w_t^r$ تولید می‌شود. سپس بردار محتوا $r_t$ مطابق رابطه ۳-۱۳ ایجاد می‌شود. نهایتا وضعیت مخفی کنترل‌گر $h_t$ وابسته به بردار محتوای حافظه $r_t$ و وضعیت مخفی پیشین کنترل‌گر $h_{t-1}$ محاسبه می‌شود و مدل برچسب خروجی $y_t$ برای ورودی را پیش‌بینی می‌کند.\cite{gulcehre2018dynamic}

\begin{equation}
r_t = M_t^Tw_t^r
\end{equation}
  
کنترل‌گر با پاک‌کردن محتوای قدیمی و نوشتن اطلاعات جدید حافظه را بروز می‌کند. کنترل‌گر سه بردار را محاسبه می‌کند: بردار پاک‌کردن $e_t$، وزن‌های نوشتن $w_t^w$ و بردار محتوای نامزد $_t\bar{c}$. وزن‌های نوشتن $w_t^w$ و خواندن $w_t^r$ با سرهای مجزا محاسبه می‌شود و با شبکه پرسپترونی چندلایه \LTRfootnote{Multi Layer Perceptran (MLP)} پیاده‌سازی می‌شود. این بردارهای وزن برای تعامل با حافظه استفاده می‌شود. بردار پاک‌کردن نیز با یک شبکه پرسپترونی ساده محاسبه می‌شود که وابسته به وضعیت مخفی کنترل‌گر $h_t$ است. بردار محتوای حافظه نامزد بر مبنای وضعیت مخفی فعلی $h_t$ و ورودی کنترل‌گر که با یک دروازه عددی $\alpha_t$ مقیاس شده است خواهد بود. $\alpha_t$ یک تابع از وضعیت مخفی و ورودی کنترل‌گر است. در رابطه ۳-۱۴ و ۳-۱۵ به ترتیب نحوه محاسبه $\alpha_t$ و $\bar{c}_t$ آورده شده است. با داشتن بردارهای پاک‌کردن، نوشتن و محتوای حافظه نامزد ماترس محتوا مطابق با رابطه ۳-۱۶ قابل بروزرسانی است.\cite{gulcehre2018dynamic}

\begin{equation}
\alpha_t = f(h_t, x_t)
\end{equation}

\begin{equation}
\bar{c}_t = ReLU(W_mh_t + \alpha_tW_xx_t)
\end{equation}

\begin{equation}
C_t[j] = (1-e_tw_t^w[j]) \odot C_{t-1}[j] + w_t^w[j]\bar{c}_t
\end{equation}

یک عمل بی‌عملی برای کنترل‌گر که برای تنها یک مرتبه در یک زمان کاری نکند می‌تواند مفید باشد. این موقعیت با طراحی یک سلول حافظه به عنوان سلول بی‌عملی اضافی مدل شده است. کنترل‌گر زمانی که نیازی به نوشتن به یا خواندن از حافظه ندارد باید به این سلول دسترسی داشته باشد چراکه نوشتن و خواندن کاملا نادیده گرفته می‌شوند. بی‌عملی برای نوشتن در ماتع پویا معادل با یادگیری آن است که دروازه نوشتن در ماتع سنتی محتوای حافظه را بدون تغییر باقی بگذارد.\cite{gulcehre2018dynamic}
\\

در تصویر ۳-۳ نمایش گرافیکی از ماتع پویا با کنترل‌گر بازشگتی نشان داده شده است.

\begin{figure}[!h]
\begin{center}
\includegraphics[height=6cm]{DNTM.png}
\end{center}
\caption{معماری مدل ماتع پویا\cite{gulcehre2018dynamic}}
\medskip
\small
کنترل‌گر یک حقیقت را به عنوان به عنوان بردار ورودی کدشده توسط شبکه عصبی بازگشتی دریافت می‌کند و وزن‌های خواندن و نوشتن برای دسترسی به حافظه را محاسبه می‌کند. اگر ماتع پویا به صورت خودکار شناسایی کند که یک کوئری دریافت شده است یک پاسخ بر می‌گرداند و کار را خاتمه می‌دهد.\cite{gulcehre2018dynamic}
\end{figure}

\subsection{مکانیسم آدرس‌دهی}
کنترل‌گر بردار کلید را مطابق با رابطه ۳-۱۷ بدست می‌آورد. سپس با کمک رابطه ۳-۱۸ وزن‌های لاجیتس\LTRfootnote{Logits} آدرس‌دهی $z_t[i]$ حاصل می‌شود. در این رابطه $S$‌ یک رابطه شباهت مانند شباهت کسینوسی(رابطه ۲-۲)است. $\beta_t$ یک فاکتور تیزی\LTRfootnote{Sharpness Factor} است که نحوه محاسبه آن در رابطه ۳-۱۹ آمده است. تابع $softmax$ مطابق با رابطه ۳-۲۰ خواهد بود.\cite{gulcehre2018dynamic} 

\begin{equation}
k_t = W_k^Th_t + b_k
\end{equation}

\begin{equation}
z_t[i] = \beta^tS(k_t, M_t[i])
\end{equation}

\begin{equation}
\beta_t = softplus(u_{\beta}^Thـt + b_{\beta}) + 1
\end{equation}

\begin{equation}
softplus(x) = log(exp(x)+1)
\end{equation}

میانگین وزن‌دار نمایی لاجیتس آدرس‌دهی $v_t$ مطابق با رابطه ۳-۲۱ محاسبه می‌شود. سپس با کمک رابطه ۳-۲۲ و ۳-۲۳ وزن‌های آدرس‌دهی $w_t$ حاصل می‌شود. با کمک این روابط وزن مربوط به سطرهایی از حافظه که اخیرا کمتر مورد استفاده قرار گرفته‌اند افزایش می‌یابد. تاثیر این رخداد با کمک $\gamma_t$ تعیین می‌شود. این مکانیسم تعمیمی بر آدرس‌دهی برپایه محتوا سنتی است.\cite{gulcehre2018dynamic}

\begin{equation}
v_t = 0.1 * v_{t-1} + 0.9 * z_t
\end{equation}

\begin{equation}
\gamma_t = sigmoid(u_\gamma^Th_t + b_\gamma)
\end{equation}

\begin{equation}
w_t = softmax(z_t - \gamma_tv_{t-1})
\end{equation}

هر سطر از ماتریس آدرس‌دهی $w_t$ دارای مقدار مثبت است که جمع آن‌ها برابر با یک است. می‌توان بردار تک‌روشن\LTRfootnote{One-Hot} $\tilde{w}_t$ را از روی آن ایجاد کرد. در زمان آموزش این بردار با رابطه ۳-۲۴ ایجاد می‌شود. این برنامه آدرس‌دهی گسسته\LTRfootnote{Discrete} و مدل هم ماتع پویای گسسته نامیده می‌شود.\cite{gulcehre2018dynamic}

\begin{equation}
\tilde{w}_t[k] = I(k=argmax(w_t))
\end{equation}

در انتهای این بخش باید تاکید کرد که در ماتع پویا و در هر گام زمانی کنترل‌گر می‌تواند چندین بیشتر از یک درخواست برای دسترسی به حافظه داشته باشد. این کار با یک گزینه اضافه انجام می‌شود. در ماتع سنتی با چندین سر این کار می‌توانست انجام شود.\cite{gulcehre2018dynamic}

\subsection{آموزش}

تابع هزینه مورد استفاده در ماتع پویا مانند رابطه ۳-۲۵ است. ماتع پویا پیوسته می‌تواند مانند ماتع سنتی با انتشار به عقب \LTRfootnote{‌Backpropagation} آموزش یابد ولی در ماتع پویا گسسته به دلیل استراتژی نمونه‌برداری در زمان آموزش این امکان وجود نخواهد داشت.\cite{gulcehre2018dynamic}

\begin{equation}
C(\theta) = 0 \frac{1}{N} \sum_{n=1}^N log\, p(y^{(n)}|x_1^{(n)},...,x_T^{(n)}; \theta)
\end{equation}

برای ماتع پویا گسسته تابع سود مطابق رابطه ۳-۲۶ تعریف می‌شود و مطابق رابطه ۳-۲۷ و ۳-۲۸ نرمال می‌شود. در رابطه ۳-۲۸ $b(x)$ یک شبکه است که برای کاهش خطای هوبر\LTRfootnote{Hubber Loss} آموزش می‌بیند. این خطا در رابطه ۳-۲۹ آورده شده است. مقدار $z$ برابر با $\bar{R}(x)$ خواهد بود.\cite{gulcehre2018dynamic}

\begin{equation}
R(x) = log\, p(y^{(n)}|x_1^{(n)},...,x_T^{(n)}; \theta)
\end{equation}

\begin{equation}
\tilde{R}(x) = \frac{R(x)-b}{\sqrt{\sigma^2+\epsilon}}
\end{equation}

\begin{equation}
\bar{R}(x) = \tilde{R}(x) - b(x)
\end{equation}

\begin{equation}
H_\delta(z) = \begin{cases}
z^2 &\text{$|z| \le \delta$} \\
\delta(2|z|-\delta) & \text{$o.w.$}
\end{cases}
\end{equation}

تابع خطا برای حالت گسسته در رابطه ۳-۳۰ آورده شده است. در این رابطه $\mathcal{H}$ نمایانگر انتروپی\LTRfootnote{Entropy} است.

\begin{multline}
C^n(\theta) = -log\,p(y|x_{1:T}, \tilde{w}^r_{1:J}, \tilde{w}^w_{1:J})\\
- {\sum_{j=1}^J{\bar{R}(x^n)(log\,p(\tilde{w}^r_j|x_{1:T})+log\,p(\tilde{w}^w_j|x_{1:T}))}}\\
- {\lambda_H \sum_{j=1}^J{\mathcal{H}(w_j^r|x_{1:T})+\mathcal{H}(w_j^w|x_{1:T})}}
\end{multline}