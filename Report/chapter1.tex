\chapter{مقدمه}

امروزه شبکه‌های عصبی\LTRfootnote{Neural Network} به شاخه اصلی مدل‌های هوش‌مصنوعی تبدیل شده‌‌اند و تقریبا در تمام کاربردهای ممکن جای خود را باز کرده‌اند. در برخی از کاربردها استفاده از اطلاعات لحظات پیشین یا ورودی‌های قبلی یک دنباله از ورودی الزامی به نظر می‌رسد. برای این کاربردها باید ساز و کاری در دل شبکه عصبی طراحی شده باشد که به هر طریق ممکن اطلاعات پیشین را خود نگه دارد. شبکه‌های عصبی بازگشتی\LTRfootnote{Recurrent Neural Network (RNN)} چنین قابلیتی را دارند.
\\

شبکه‌های عصبی بازگشتی دروازه‌دار\LTRfootnote{Gated Recurrent Neural Network (Gated RNN)} نظیر \lr{LSTM} اگرچه قادر بودند به واسطه یک بردار داخلی اطلاعات پیشین را حفظ کنند ولی این حافظه محدود است و زمانی‌که نیاز به نگهداری اطلاعات با حجم بالا به وجود بیاید ناتوانی این دست از شبکه‌ها عیان می‌شود. در این موارد وجود یک حافظه خارجی می‌تواند راهگشا باشد.
\\

در سال‌های جدید دو معماری شبکه عصبی برای حل این مشکل ارائه شده است که از یک حافظه در کنار شبکه عصبی بهره گرفتند:
\begin{enumerate}
\item \textbf{شبکه حافظه‌ای}\LTRfootnote{Memory Network}: شبکه حافظه‌ای به صورت صریح تمام اطلاعات یا حقایق را که در هر دوره وجود دارد را در یک حافظه خارجی خارجی ذخیره می‌کنند و از مکانیسم مبتنی بر توجه زمانی که قصد شاخص‌گذاری آن‌ها در زمان محاسبه یک خروجی را دارند استفاده می‌کنند.
\item \textbf{ماشین‌ تورینگ عصبی}\LTRfootnote{Neural Turing Machine}: ماشین تورینگ عصبی  که در ادامه آن را به اختصار ماتع می‌نامیم هر حقیقت را در یک دوره می‌خواند و تصمیم می‌گیرد که آیا آن را در حافظه خارجی قابل تمایز بنویسد، بخواند و یا هر دو کار را انجام دهد یا نه.\cite{gulcehre2018dynamic}
\end{enumerate}

تفاوت مهم بین دو مدل این است که شبکه‌های حافظه‌ای مکانیسمی برای تغییر محتوای حافظه خارجی را ندارد درحالی که ماتع‌ها این قابلیت را دارند. در عمل این مسئله منجر به یادگیری ساده‌تر برای وظایف واقعی در شبکه حافظه‌ای می‌شود. در مقابل، ماتع در ابتدای معرفی عمدتا بر روی یک سری از وظایف ساختگی  با مقیاس کوچک نظیر رونوشت‌گیری\LTRfootnote{Copy} و یادآوری انجمنی\LTRfootnote{Association Recall} ارزیابی شده است. هرچند ماتع بیان دقیق‌تری دارد؛ چراکه می‌تواند وضعیت داخلی شبکه و همچنین فرآیند‌های یک دوره را ذخیره کند یا تغییر دهد و ما را قادر خواهد ساخت که از آن بدون هیچگونه تغییری بر مدل برای وظایف مختلف استفاده کنیم.\cite{gulcehre2018dynamic}
\\

ماتع در سال ۲۰۱۴ توسط گروهی از محققین گوگل به سرپرستی گریوز\LTRfootnote{Graves} ارائه شد \cite{graves2014neural}. تعداد ارجاعات به این مقاله تا بدین لحظه از دو هزار گذشته است. باتوجه به قدرت بالقوه‌ بالایی که یک ماتع دارد در این پروژه قصد داریم آن را مورد بررسی قرار دهیم. طبیعتا اگر یک شبکه عصبی بتواند به درستی با یک حافظه بزرگ تعامل برقرار کند، پیشرفت قابل ملاحظه‌ای در حوزه شبکه‌های عصبی رخ خواهد داد.
\\

سوال مهمی که باید به آن پاسخ داده شود آن است که آیا ماتع‌ها توانسته‌اند از قدرت بالقوه خود بهره بگیرند یا نه. پیش از پاسخ به این سوال باید مجددا یادآوری کنم که ماتع در مقاله اولیه خود برای حل چندین وظیفه ساختگی ساده مورد ارزیابی قرار گرفته است؛ به عنوان مثال در وظیفه رونوشت‌گیری به طور کلی هدف آن است که ورودی مستقیما در حافظه نوشته شود و در انتها از حافظه خوانده شود و به عنوان خروجی برگردانده شود. ناگفته پیداست که کاربردهای واقعی تا این میزان سرراست نیستند و جواب مناسب بر روی وظیفه‌هایی از این دست نمی‌تواند نشان از موفقیت ماتع در مسائل واقعی باشد.
\\

مسئله تنها به نتیجه خروجی ختم نمی‌شود. پیچیدگی‌های طبیعی این شبکه مانع مهمی برای استفاده و توسعه آن است. توسعه‌دهندگان ماتع پیاده‌سازی و جزئیات کافی برای پیاده‌سازی خود را ارائه ندادند\cite{collier2018implementing}. این مسئله قطعا تاثیر منفی‌ای برای پیشرفت این شبکه به نسبت پیچیده بود. پیاده‌سازی‌های منبع‌باز\LTRfootnote{Open Source} اولیه آن یا سرعت پایینی داشتند و یا ممکن بود وزن‌های آن به بی‌نهایت میل کند و آموزش دچار مشکل شود. نهایتا در سال ۲۰۱۸ یعنی چهار سال بعد از معرفی ماتع کولیر\LTRfootnote{Collier} و بیل \LTRfootnote{Beel} با یک پیاده‌سازی مناسب و منبع‌باز توانستند نتایج مقاله اصلی را در زمان مناسب تکرار کنند.\cite{collier2018implementing}. به گفته آنان مقداردهی اولیه\LTRfootnote{Initialization} حافظه نقش مهم در رسیدن به پیاده‌سازی مناسب آن‌ها داشته است.\cite{collier2018implementing}.
\\

با تفاسیر بیان‌شده شاید کاربردی بودن این شبکه مورد تردید باشد. اما باید گفت که افزونه‌های ماتع\cite{gulcehre2018dynamic}\cite{merrild2018hyperntm}\ در سال‌های بعد بهبودهای مهمی بر روی نسخه اولیه اعمال کردند. به علاوه مدل‌های زیادی بر پایه ماتع برای مسائل واقعی و مجموعه‌داده‌های متعارف استفاده شده است که نتایج خوبی را رقم زده است. در این گزارش پس از بررسی ماتع سنتی افزونه‌های آن معرفی می‌شود و با ارائه کاربرهای واقعی و بخشی از نتایج مرتبط به کاربرهای واقعی نشان خواهیم داد که ماتع شبکه‌ای خلاقانه و در عین حال کاربردی است. ادامه ساختار این پروژه به شرح زیر است:
\begin{itemize}
\item در بخش دوم ساختار یک ماتع و نحوه آموزش آن تشریح خواهد شد. وظایف ساختگی و اولیه‌ای که یک ماتع برای حل آن مناسب است بیان می‌شود.
\item در بخش سوم چند تا از افزونه‌های ماتع بررسی می‌شود. باتوجه به محدودیت‌های این پروژه تنها شماری از این افزونه‌ها معرفی می‌شود. در این بخش ساختار و روال آموزش هر افزونه شرح داده خواهد شد.
\item در بخش چهارم کاربردهای واقعی که ماتع و افزونه‌های آن توانسته‌اند در آن استفاده شوند ارائه می‌شود. برای برخی از کاربردها ماتع و افزونه‌های آن تغییراتی داشته است؛ در این موارد تغییرات اساسی تبیین می‌شود.
\item در بخش پنجم بخشی از برترین نتایج گزارش‌شده در مقالات که مدلی بر پایه ماتع به دقت‌های خوبی رسیده است گلچین شده است.
\item نهایتا بخش ششم مروری بر مطالب این پروژه خواهد بود.  
\end{itemize}