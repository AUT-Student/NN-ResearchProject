# حافظه
![[Pasted image 20220501122115.png]]
مدل D-NTM‌ مدل NTM را گسترش داده است. مدل D-NTM شامل دو ماژول اصلی است: کنترل‌گر و حافظه. کنترل‌گر را معمولا با RNN پیاده‌سازی می‌کنند. این ماژول درخواست خواندن از، نوشتن به یا پاک کردن زیرمجموعه‌ای از سلول‌های حافظه را می‌دهد.

![[Pasted image 20220501122459.png]]
![[Pasted image 20220501122519.png]]
D-NTM شامل یک حافظه خارجی $M_t$ است که هر سلول حافظه i در $M_t[i]$ به دو بخش شکسته است: 
یک بردار آدرس آموزش‌پذیر 
$$ A_t[i]\in R^{1×d_n}$$ یک بردار ثابت
$$ C_t[i] \in R^{1×d_c}$$
که برای آن‌ها خواهیم داشت:
$$M_t[i] = [A_t[i];C_t[i]]$$
حافظه $M_t$ شامل N  سلول حافظه می‌شود و بنابراین آن را با یک ماتریس مستطیلی 
$$ M_t \in R^{N×(d_c+d_a)}$$
نشان می‌دهد. و خواهیم داشت:
$$M_t = [A_t;C_t]$$
$A_t$ قابل آموزش است و $C_t$ یک ماتریس ثابت است. بخش آدرس $A_t$ پارامتر مدل است که در طول یادگیری بروز می‌شود. در حین استنتاج بخش آدرس توسط کنترل‌گر تغییر پیدا نمیکند و ثابت می‌ماند. بخش محتوا $C_t$ در حین آموزش و استنتاج توسط کنترل‌گر چه برای نوشتن و چه برای خواندن تغییر پیدا می‌کند.
در ابتدای هر دوره بخش محتوای حافظه به یک ماتریس تمام صفر تغییر پیدا می‌کند: $C_0=0$ این شروع باعث می‌شود که بخش قابل آموزش آدرس برای هر سلول حافظه به مدل امکان یادگیری استراتژی‌های آدرس‌دهی پیچیده مبتنی بر مکان را بدهد.

# کنترل‌گر
![[Pasted image 20220701182633.png]]
1) در گام زمانی t کنترل‌گر یک مقدار ورودی $x_t$ دریافت می‌کند. 
2) به حافظه دسترسی پیدا می‌کند و آن را می‌خواند و بردار محتوای $r_t$ را ایجاد می‌کند
3) یک بخشی از اطلاعات را روی حافظه می‌نویسد.
4) وضعیت تصادفی خود را بروز می‌کند.
5) در صورت نیاز مقدار $y_t$ را به عنوان خروجی می‌دهد.

در این مقاله هم از GRU و هم از شبکه‌های جلورو برای پیاده‌سازی کنترل‌گر استفاده شده است. 

برای کنترل‌گر GRU داریم:
$$h_t = GRU(x_t, h_{t−1},r_t )$$
برای کنترل‌گر جلورو داریم:
$$h_t = \sigma(x_t, r_t)$$
# عملیات مدل
![[Pasted image 20220701183345.png]]
در گام زمانی t کنترل‌گر $x_t$‌ را به عنوان ورودی می‌گیرد و سپس وزن خواندن 

$$ w_t^r \in R^{N×1}$$
تولید می‌شود. با استفاده از وزن خواندن $w_t^r$ بردار محتوا از حافظه خوانده می‌شود و بدین ترتیب محاسبه می‌شود:
$$r_t \in R^{(d_a+d_c)×1}$$
$$r_t = (M_t)^T w_t^r$$
![[Pasted image 20220701184057.png]]
وضعیت مخفی کنترل‌گر ($h_t$) وابسته به بردار محتوای حافظه $r_t$ و وضعیت مخفی پیشین کنترل‌گر است. مدل برچسب خروجی $y_t$ برای ورودی را پیش‌بینی می‌کند.

![[Pasted image 20220701184323.png]]
کنترل‌گر با پاک‌کردن محتوای قدیمی و نوشتن اطلاعات جدید حافظه را بروز می‌کند. کنترل‌گر سه بردار را محاسبه می‌کند:
1) بردار پاک کردن $$e_t \in R^{d_c×1}$$ 
2) وزن‌های نوشتن $$w_t^w \in R^{N×1}$$
3) بردار محتوای نامزد ???

اما وزن‌های نوشتن $w_t^w$ و خواندن $w_t^r$ با سرهای مجزا محاسبه می‌شود و با MLP پیاده‌سازی می‌شود و این بردارهای وزن برای تعامل با حافظه استفاده می‌شود. بردار پاک‌کردن با یک MLP ساده محاسبه می‌شود که وابسته به وضعیت مخفی کنترل‌گر $h_t$ است. بردار محتوای حافظه نامزد ؟؟؟ بر مبنای وضعیت مخفی بعلی $h_t$ و ورودی کنترل‌گر که با یک گیت عددی $\alpha_t$ مقیاس شده است خواهد بود. $\alpha_t$ یک تابع از وضعیت مخفی و ورودی کنترل‌گر است:
![[Pasted image 20220701190745.png]]
![[Pasted image 20220701190821.png]]
در رابطه ۲.۴ و ۲.۵ $w_m$ و $w_x$ ماتریس‌های آموزش‌پذیر است. با داشتن بردارهای پاک‌کردن، نوشتن و محتوای حافظه نامزد، ماتریس حافظه با رابطه ۲.۶ بروز می‌شود. در این رابطه $C_t[j]$ j-امین سطر از ماتریس محتوا از ماتریس حافظه است.

![[Pasted image 20220701191356.png]]

بی‌عملی. یک بی‌عملی اضافی (NOP) برای کنترل‌گر که برای تنها یک مرتبه در یک زمان کاری نکند می‌تواند مفید باشد. ما این موقعیت را با طراحی یک سلول حافظه به عنوان سلول بی‌عملی اضافی مدل کردیم. کنترل‌گر زمانی که نیازی به نوشتن به یا خواندن از حافظه را ندارد باید به این سلول دسترسی داشته باشد چراکه نوشتن و خواندن کاملا نادیده گرفته می‌شوند.

![[Pasted image 20220701193807.png]]
![[Pasted image 20220701193735.png]]
در مورد عمل خواندن و نوشتن در D-NTM اطلاعات بیشتری در تصویر ۱ قرار دارد.

**تصویر ۱**: نمایش گرافیکی از روش D-NTM‌ ارائه شده با کنترل‌گر بازگشتی. کنترل‌گر یک حقیقت را به عنوان به عنوان بردار ورودی کدشده توسط شبکه عصبی بازگشتی دریافت می‌کند و وزن‌های خواندن و نوشتن برای دسترسی به حافظه را محاسبه می‌کند. اگر D-NTM به صورت خودکار شناسایی کند که یک کوئری دریافت شده است یک پاسخ بر می‌گرداند و کار را خاتمه می‌دهد.

![[Pasted image 20220701194340.png]]
بی‌عملی برای نوشتن در D-NTM معادل با یادگیری آن است که دروازه نوشتن در NTM محتوای حافظه را بدون تغییر باقی بگذارد.

محاسبات ماتریس‌های خواندن $w_t^r$ و نوشتن $w_t^w$ بخش حساس مدل است چراکه کنترل‌گر باید تصمیم بگیرد آیا با کمک آن‌ها از حافظه بخواند و در حافظه بنویسد. این مورد در بخش ۳ تشریح می‌شود.  