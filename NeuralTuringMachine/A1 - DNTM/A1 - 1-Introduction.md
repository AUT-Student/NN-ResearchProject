# پاراگراف اول

![[Pasted image 20220430092136.png]]
![[Pasted image 20220430092150.png]]

برخلاف پیشرفت‌های اخیر در یادگیری عمیق یک تعداد از وظایف چالشی همچنان به خوبی توسط شبکه‌های عصبی عام‌منظوره به خوبی حل نشده است. این وظایف اغلب به یک یک شبکه عصبی مجهز به یک حافظه خارجی نیاز دارد که این حافظه خارجی محلی است که دانسته‌های بزرگ‌تر و بدون مرز را در آن نگهداری شود. مانند پرسش و پاسخ دوره‌ای (episodic question-answering)، یادگیری الگوریتم‌های فشرده (Learning of compact algorithms)، محاوره (Dialogue) و تولید عنوان ویدئو (Video caption generation) است.
این وظایف شامل هر دوی وابستگی بازه طولی (Long-range) است که یادگیری را برای شبکه‌های عصبی بازگشتی سنتی سخت می‌کند و به مدل‌هایی که استدلال پیچیده‌تری روی داده انجام بدهند نیاز است.

# پاراگراف دوم
![[Pasted image 20220430094359.png]]
اخیرا دو معماری شبکه عصبی برای حل این وظایف ارائه شده است که نیاز به یک حافظه خارجی دارند:
۱) شبکه‌های حافظه‌ای (Memoty Netwols) به صورت صریح تمام اطلاعات یا حقایق را که در هر دوره وجود دارد را درر یک حافظه خارجی خارجی ذخیره می‌کنند و از مکانیسم مبتنی بر توجه زمانی که قصد شاخص گذاری آن‌ها در زمان محاسبه یک خروجی را دارند استفاده می‌کنند.
۲) ماشین‌های تورینگ عصبی (NTM) که هر حقیقت را در یک دوره می‌خواند و تصمیم می‌گیرد که آیا آن را در حافظه خارجی قابل تمایز بنویسد، بخواند و یا هر دو کار را انجام دهد.

# پاراگراف سوم
![[Pasted image 20220430094936.png]]
یک تفاوت جدی بین دو مدل این است که شبکه‌های حافظه‌ای مکانیسمی برای تغییر محتوای یک حافظه خارجی ندارد درحالی که ماشین‌های تورینگ عصبی دارند. در عمل این مسئله منجر به یادگیری ساده‌تر در شبکه حافظه‌ای می‌شود برای وظایف واقعی می‌شود.
در مقابل، ماشین تورینگ عصبی عمدتا بر روی یک سری از وظایف ساختگی  با مقیاس کوچک نظیر کپی‌کردن و یادآوری انجمنی (Associative Recall) تست شده است. اگرچه ماشین تورینگ عصبی بیان دقیق‌تری دارد؛ چراکه آن می‌تواند وضعیت داخلی شبکه و همچنین فرآیند‌های یک دوره را ذخیره کند و تغییر دهد و ما قادر خواهیم بود از آن بدون هیچگونه تغییری بر مدل برای وظایف مختلف استفاده کنیم.


# پاراگراف چهارم
![[Pasted image 20220430095935.png]]

ماشین تورینگ عصبی اصلی دو مدل آدرس‌دهی را پشتیبانی می‌کند که می‌تواند به صورت همزمان استفاده شود: آدرس‌دهی بر پایه محتوا و آدرس‌دهی بر پایه موقعیت. استراتژی برپایه موقعیت خود بر پایه آدرس‌دهی خطی است که فاصله بین یک جفت از سلول حافظه متوالی همواره برابر با یک مقدار ثابت است. ما این محدودیت را با معرفی معرفی یک بردار آدرس‌دهی قابل یادگیری برای هر سلول حافظه در شبکه تورینگ عصبی که اخیرا به عنوان مکانیسم آدرس‌دهی حافظه استفاده شده است حل کرده‌ایم. ما نام این افزونه را ماشین تورینگ عصبی پویا نامیده‌ایم.

# پاراگراف پنجم
![[Pasted image 20220501090928.png]]
مدل با وظایف مجموعه‌داده Facebook bAbI هم با استفاده از توجه پیوسته قابل تمایز و هم با استفاده از توجه گسسته غیرقابل تمایز ارزیابی شده است. نتایج نشان می‌دهد که این امکان وجود دارد که از مکانیسم توجه گسسته غیرقابل تمایز استفاده کرد و در اصل D-NTM‌با توجه گسسته و کنترلگر gated recurrent unit شبکه با توجه پیوسته را شکست می‌دهد. 
به علاوه نتایج بر روی sequential pMNIST و SNLI و وظایف الگوریتمی ارزیابی شده است تا نشان دهند که مدل قادر است وابستگی‌های طولانی‌مدت را هم مدیریت کند.

# پاراگراف ششم
![[Pasted image 20220501090959.png]]
![[Pasted image 20220501091014.png]]
مشارکت این مقاله شامل موارد زیر بوده است:
* یک نسخه از ماشین تورینگ عصبی به نام ماشین تورینگ عصبی پویا ارائه شده است.
* نشان داده‌اند که استفاده از NTMها در بیشتر وظایف طبیعی مانند پاسخ پرسش دوره‌ای، Natural Language Entailment و دسته‌بندی رقمی پیکسل‌ها ممکن است. یک تحلیل با جزئیات از مدلشان روی تسک bAbI ارائه شده است.
* استفاده از مکانیسم توجه را نشان داده‌اند و به صورت تجربی نشان داده‌اند که این مدل می‌تواند آدرس‌دهی مبتنی برتوجه پیوسته را برای یک وظیفه پرزش پاسخ دوره‌ای شکست دهد.
* آن‌ها یک استراتژی برای مدلشان به همراه کنترل‌گر پیشرو و مکانیسم توجه گسسته ارائه داده‌اند که نتایج را به طرز قابل ملاحظه‌ای بهبود می‌دهد.

# پاراگراف هفتم
![[Pasted image 20220501115923.png]]
آن‌ها مقایسه‌ای از مدل پیشنهادی و مدل‌های مشابه نظیر NTM و LSTM با شرایط یکسان انجام داده‌اند که این موضوع کمک به درک بهتر شکست مدل می‌کند.

# پاراگراف هشتم
![[Pasted image 20220501120059.png]]
مابقی مقاله به شرح زیر است:
* بخش دوم: معماری D-NTM‌ ارائه می‌شود.
* بخش سوم: مکانیسم آدرس‌دهی برای D-NTM توصیف می‌شود.
* بخش چهارم: فرآیند آموزش تشریح می‌شود.
* بخش پنجم: مدل‌های مرتبط توصیف می‌شود.
* بخش ششم: نتایج روی وظیفه پاسخ پرسش دوره‌ای گزارش می‌شود.
* بخش هفتم: نتایج روی sequential MNIST
* بخش هشتم: نتایج روی SNIL
* بخش نهم: نتایج روی وظایف یادگیری مصنوعی
* بخش دهم: نتیجه‌گیری