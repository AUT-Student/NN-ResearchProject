# تکامل عصبی توپولوژی‌های تقویت‌کننده
![[Pasted image 20220701092519.png]]
روش HyperNEAT امکان یادگیری از هندسه را در این مقاله فراهم کرده است. این روش یک افزونه از NEAT است که شبکه‌های عصبی را با کدگذاری مستقیم تکامل می‌دهد.

این روش با یک جمعیت از شبکه‌های عصبی ساده شروع می‌کند و سپس آن‌ها را در طی نسل‌ها با افزودن راس‌های جدید و اتصالات به کمک جهش پیچیده‌تر می‌کند. با تکامل شبکه‌ها از این را لازم نیست توپولوژی شبکه‌ها از پیش دانسته شده باشد. NEAT به طرز فزاینده‌ای در شبکه‌های پیچیده جستجو می‌کند تا یک سطح مناسب از پیچیدگی را پیدا کند.

ویژگی مهم NEAT برای هدف این مقاله آن است که هم توپولوژی و هم وزن‌های یک شبکه را تکامل می‌دهد. چراکه آن به سادگی و تدریجی پیچیدگی را افزایش می‌دهد و این باعث می‌شود که یک شبکه مناسب با اندازه مینیمال حاصل شود.

# HyperNEAT
![[Pasted image 20220701093736.png]]
در کدگذاری‌های مستقیم مانند NEAT هر بخش از نمایش جواب به یک تکه کوچک از ساختار نهایی جواب نگاشت می‌شود. عیب مهم این روش آن است که بخش‌های مختلف راه حل به یکدیگر شبیه هستند باید کد شوند و جداگانه کشف شوند. در این مقاله به جای آن از کدگذاری غیرمستقیم استفاده می‌کند؛ یعنی توصیف راه‌حل فشرده می‌شود و چنین اطلاعاتی می‌تواند کاهش بیابد. کدگذاری غیرمستقیم به دلیل امکان آنکه یک راه‌حل به شکل الگویی از پارامترها نمایش یابد و نه آنکه هر پارامتر جداگانه نمایش یابد قدرت‌مند است.

HyperNEAT‌در این بخش مرور شده به عنوان یک افزونه کدگذار غیرمستقم از NEAT که در تعدادی از حوزه‌های چالش‌بر‌انگیز که نیازمند کشف منظم‌سازی‌هایی بوده است اثبات شده است.

![[Pasted image 20220701093803.png]]
در HyperNEAT یک NEAT به جای شبکه‌های عصبی با یک کدگذاری غیرمستقیم به نام CPPN کدگذاری می‌شود. CPPN همزمان که شبکه هستند برای کدگذاری ترکیب توابع طراحی شده‌اند که هر تابع در ترکیب آزادانه مرتبط با یک منظم‌سازی مفید است.

![[Pasted image 20220701094026.png]]
![[Pasted image 20220701094756.png]]
نکته مفید این کدگذار آن است که به الگوهای مکانی اجازه می‌دهد که به عنوان شبکه‌هایی از توابع ساده نمایش پیدا کنند. (مثلا CPPN‌ها)؛ این یعنی NEAT می‌تواند با CPPN مانند شبکه‌های عصبی تکامل پیدا کند. CPPN‌ها مشابه شبکه‌های عصبی هستند اما آن‌ها متکی بر بیشتر از یک تابع فعال‌سازی هستند و آن‌ها یک توسعه زیستی انتزاعی به جای یک مغز هستند.
کدگذاری غیرمستقیم CPPN می‌تواند به طور فشرده الگوها با نظم‌هایی نظیر تقارن، تکرار و تکرار با تغییر را کد کنند. 

![[Pasted image 20220701094814.png]]
به عنوان مثال با انتخاب یک تابع گاوسین که خاصیت تقارن دارد الگوی خروجی به سادگی قرینه خواهد شد. یک تابع دوره‌ای مانند سینوس در حین تکرار قطعه‌سازی انجام می‌دهد. تکرار با تغییر (مانند انگشتان دست) به سادگی با ترکیب یک فریم منظم (مانند سینوس یا گاوسین) با یک فریم نامنظم (مانند محور x نامتقارن) بدست می‌آید. پتانسیل CPPN نمایش الگوها مشابه با الگوهای ارگانیسم‌های طبیعی که در چندین مطالعه نشان داده شده است است.

![[Pasted image 20220701101349.png]]
ایده اصلی HyperNEAT آن است که CPPN‌ها می‌تواند اتصال الگوهای را کدکند. بدین طریق NEAT می‌تواند CPPN که شبکه‌های عصبی بزرگ با منظم‌سازی‌ها و تقارن‌های خود نمایش می‌دهد را تکامل دهد.

![[Pasted image 20220701101633.png]]
![[Pasted image 20220701101648.png]]
سابقا CPPN ها توابع هندسی بودند که الگوهای اتصال خروجی آن رئوسی در n بعد بود که n تعداد ابعاد در فضای کارتزین است. یک CPPN که چهار خروجی برچسب‌خورده $x_1$، $y_1$، $x_2$ و $y_2$ را دریافت کند. این نقطه در فضای چهار بعدی همچنین با اتصال بین نقاط دوبعدی $(x_1, y_1)$ و $(x_2, y_2)$ و خروجی CPPN برای ورودی نامیده می‌شود. بنابراین مطابق تصویر ۱ وزن ارتباطات نمایش داده می‌شود. با کوئری زدن هر هر اتصال ممکن در مجموعه‌ی از پیش انتخاب‌شده نقاط در این روش یک CPPN می‌تواند یک شبکه عصبی ایجاد کند که هر نقطه کوئری‌زده‌شده یک مکان عصبی خواهد بود. چراکه اتصالات با یک تابع از خروجی‌های آن تولید می‌شود و ساختارنهایی با دانش آن گراف ایحاد می‌شود.


![[Pasted image 20220701112126.png]]
**تصویر۱**: تفسیر الگوی اتصال هندسی برپایه ابرمکعب. یک مجموعه‌ای گره‌ها یا زیردرخت به مختصات بین -۱ تا +۱ در تمام ابعاد نظیر می‌شود.
(۱): هر اتصال ممکن در یک زیردرخت کوئری زده شده است تا مجاورت و وزن آن مشخص شود. خطوط جهت‌دار تیره در زیردرخت نمایش‌داده شده در تصویر یک نمونه از اتصالاتی است که کوئری زده شده است.
(۲): به صورت داخلی یک CPPN تکامل‌یافته یک گراف است که مشخص می‌کند کدام توابع فعال‌سازی متصل هستند.  همان‌طور که در شبکه عصبی اتصالات وزن‌دهی می‌شوند که خروجی یک تابع با چه وزنی به طرف دیگر اتصال برود. برای هر کوئری CPPN جایگاه دو سر اتصال را به عنوان ورودی می‌گیرد و وزن اتصال را به عنوان خروجی می‌دهد.(۳) بنابراین CPPN می‌تواند الگوهای منظم از اتصالات در فضا را تولید کند.

 
![[Pasted image 20220701101709.png]]
در HyperNEAT آزمایشگر مکان و قانون هر گره را تعریف می‌کند. به عنوان یک قانون سر انگشتی گره‌ها در لایه‌ای قرار می‌گیرد که هندسه آن وظیفه را نشان دهد. بنابراین اتصال زیرلایه یک تابع از ساختار وظیفه است. نحوه ادغام این تنظیم با یک شبکه عصبی که حافظه خارجی دارد یک مسئله باز است که این مسئله تلاش برای حل آن دارد.

# ENTM
![[Pasted image 20220701115916.png]]
برپایه اصول NTM مدل ENTM معرفی شده است که از NEAT استفاده می‌کند تا توپولوژی و وزن‌های شبکه عصبی کنترل‌گر را یاد بگیرد. از این طریق برای توپولوژی شبکه نیاز به دانش زمینه‌ای نیست (برخلاف NTM استاندارد) و شبکه می‌تواند باتوجه به پیچیدگی وظیفه رشد پیدا کند. ENTM اغلب توپولوژی‌‌های فشرده را برای حل یک وظیفه خاص پیدا می‌کند. در نتیجه جلوی جستجو غیرضروری در فضای با ابعاد بالا گرفته می‌شود. به علاوه ENTM قادر به جل مسائل یادگیری مستمر پیچیده باشد. چراکه شبکه نباید از مشتق استفاده کند و آن می‌تواند از توجه سخت و مکانیسم جابجایی استفاده کند که امکان تعمیم خوب برای دنباله‌های بلند در وظیفه رونوشت‌گیری را فراهم می‌کند. به علاوه یک نوار پویا و از نظر تئوری با اندازه بی‌نهایت اکنون قابل استفاده است.

![[Pasted image 20220701121302.png]]
ENTM یک سر تکی ترکیبی خواندن/نوشتن دارد. این شبکه بردار نوشتن $w$ با اندازه $M$، ورودی کنترل درون‌یابی نوشتن $i$، ورودی کنترل پرش محتوای $j$ و سه ورودی کنترل جابجایی $s_l$ (جابجایی چپ)، $s_o$ (بدون جابجایی) و $s_r$ (جابجایی راست) را منتشر می‌کند. اندازه بردار نوشتن $M$ اندازه هر خانه حافظه بر روی نوار را مشخص می‌کند. جزء درون‌یابی نوشتن امکان مخلوط‌کردن مقادیر فعلی نوار و بردار نوشتن در موقعیت نوشتن را فراهم می‌کند. $M_h(t)$ محتوای نوار در موقعیت سر فعلی $h$ در زمان $t$، $i_t$ درون‌یابی نوشتن و $w_t$ بردار نوشتن است. برای تمام این‌ها در زمان $t$ خواهیم داشت: 
$$M_h(t) = M_h(t − 1) · (1 − i_t) + w_t · i_t.$$
![[Pasted image 20220701133511.png]]
پرش محتوا مشخص می‌کند که آیا سر باید به موقعیتی در حافظه حرکت کند که بیشترین شباهت را به بردار نوشتن دارد یا نه. یک پرش محتوا اجام می‌شود اگر مقدار ورودی کنترل از ۰.۵ بیشتر شود. شباهت بین بردار نوشتن $w$ و بردار حافظه $m$‌ با رابطه زیر حساب می‌شود:
$$s(w, m) = \frac{\sum_{i=1}^{M} |w_i−m_i| }{M} $$
![[Pasted image 20220701133541.png]]
در گام زمانی $t$ اقدامات زیر به ترتیب انجام می‌شود:
1) بردار نوشتن $w_t$ برای موقعیت فعلی $h$ بدست می‌آید. این بردار با محتوای موجود باتوجه به درون‌یابی نوشتن $i_t$ درون‌یابی می‌شود.
2) اگر ورودی کنترل پرش محتوا $j_t$ بزرگ‌تر از ۰.۵ شود، سر به مکانی در نوار که بیشترین شباهت به بردار نوشتن $w_t$ دارد حرکت می‌کند
3) سر به یک موقعیت چپ‌تر، راست‌تر روی نوار حرکت می‌کند یا در همان جا ثابت می‌ماند که این وابسته به مقادیر ورودی کنترل جابجایی $s_l$، $s_0$  و $s_r$ است.
4) مقادیر نوار را در موقعیت جدید سر می‌خواند و بر می‌گرداند.