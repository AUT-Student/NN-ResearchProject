# مقداردهی اولیه محتوای حافظه
![[Pasted image 20220630115041.png]]
![[Pasted image 20220630115052.png]]
ما فرض کرده‌ایم که چگونگی تعریف محتوای اولیه حافظه یک فاکتور مهم در موفقیت پیاده‌سازی NTM است. ما سه برنامه مقداردهی اولیه حافظه را مقایسه کرده‌ایم که در پیاده‌سازی‌های منبع‌باز NTM موجود بوده است. به طور خاص ما مقداردهی اولیه ثابت را مقایسه کرده‌ایم که تمام مکان‌های حافظه را برابر با $10^{-6}$ قرار می‌دهد و مقداردهی تصادفی که هر خانه حافظه را برابر با یک مقدار از توریع نرمال کوتاه‌شده  (truncated) با میانگین صفر و انحراف معیار ۰.۵ قرار می‌دهد.

ما پنج تا از هفت پیاده‌سازی را مشاهده کردیم که از مقداردهی تصادفی استفاده می‌کردند. ما همچنین یک پیاده‌سازی را مشاهده کردیم که از مقداردهی ثابت استفاده می‌کرد و نهایتا یک پیاده‌سازی که مقداردهی اولیه را یادمی‌گرفت.

![[Pasted image 20220630120005.png]]
* مقداردهی اولیه ثابت این مزیت را دارد که نیازی به پارامترهای اضافه و ارائه یک مقداردهی اولیه حافظه پایدار در حین استنتاج ندارد. 
* مقداردهی اولیه یادگرفته‌شده مزیت بالقوه‌ای دارد که یادگیری مقداردهی اولیه بتواند یک برنامه آدرس‌دهی غیرخطی پیچیده‌ای را بر خلاف مقداردهی اولیه پایدار فعلی فعال کند. این نیاز به هزینه N×W پارامتر اضافه دارد.
* مقداردهی اولیه تصادفی مزیت بالقوه‌ای دارد که می‌تواند به عنوان یک منظم‌ساز (Regularizer) عمل کند اما این امکان هم وجود دارد که در حین استنتاج محتوای حافظه وارد محلی شود که در حین یادگیری با آن مواجه نشده بود.

# سایر پارامترهای مقداردهی اولیه
![[Pasted image 20220630120611.png]]
به جای مقداردهی اولیه به بردار خواندن $r_0$ و وزن‌های آدرس $w_0$ ، ما این مقادیر اولیه را پس انتشار می‌دهیم و آن‌ها را با یک بردار بایاس یادگرفته‌شده مقداردهی اولیه می‌کنیم.

ما معتقدیم که این برنامه مقداردهی اولیه تعمیم کافی برای وظایفی که نیازمند مقداردهی اولیه با انعطاف بیشتر با هزینه کمی در پارامترهای اضافی را ایجاد می‌کند. تعداد پارامترهای اضافی برابر است با $W ∗ H_r + N ∗ (H_r + H_w)$  که $H_r$ تعداد سرهای خواندن و $H_w$ تعداد سرهای نوشتن است.

به عنوان مثال اگر یک NTM با چندین سر نوشتن بخواهد روی موقعیت‌های مختلف حافظه در یک زمان و با استفاده از آدرس‌دهی وابسته به مکان بنویسد، $W_0$ باید برای هر هد نوشتن متفاوت مقداردهی شده باشد. نیاز به hard code کردن برای هر وظیفه یک بار اضافی بر روی مهندس است؛ به خصوص زمانی که نیاز برای چنین آدرس‌دهی‌ای ممکن است برای یک وظیفه خاص شناخته نشده باشد. بنابراین ما باید اجازه دهیم تا شبکه خود مقداردهی اولیه را بیاموزد.

# معیار شباهت
![[Pasted image 20220630120804.png]]
برای معیار شباهت از شباهت کسینوسی استفاده می‌کنند.

# ورودی‌های کنترل‌گر
![[Pasted image 20220630120843.png]]
در هر زمان الحاق ورودی که داخل NTM می‌آید و بردارهای خواندن از تمام سرهای خواندن NTM به کنترل‌گر NTM داده می‌شود. 
ما متوجه شدیم که چنین تنظیمی بهترین کارایی برای کدگذار-کدگشا‌های برپایه توجه در ترجمه ماشین عصبی را دارد.

# پارامترهای غیرخظی
![[Pasted image 20220630121311.png]]
* مشابه LSTM ما محتوای ماتریس حافظه را در رنج -۱ تا +۱ قرار دادیم. با اعمال تابع تانژانت هایپربولیک به خروجی‌های کنترل‌گر مرتبط با $k_t$ و $a_t$ و اعمال تابع سیگموید به بردار پاک‌کردن $e_t$. 

* برای برآورده کردن شرط βt ≥ 0 از تابع زیر استفاده کردیم:
$$softplus(x) \leftarrow log(exp(x)+1)$$
* ما تابع لاجستیک سیگموید را برای برآورده کردن شرط gt ∈ [0, 1] استفاده کردیم.
* از تابع softmax برای آنکه بردار شیفت کانوولوشنی $s_t$ یک توزیع احتمالاتی معتبر باشد استفاده کردیم.
* برای برآورده کردن γt ≥ 1 ابتدا تابع softplus را اعمال کردیم و سپس با یک جمع زدیم.